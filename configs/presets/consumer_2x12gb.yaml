# Preset tuned for two 12GB consumer GPUs (DistributedDataParallel).
defaults:
  - override /data: imagenet
  - override /train: default
  - override /runtime: default

data:
  batch_size: 128  # per-GPU batch size (global effective batch doubles under DDP)
  num_workers: 4
  prefetch_factor: 2
  persistent_workers: true

train:
  amp: true
  amp_dtype: bf16
  accum_steps: 2  # effective global batch 512 when combined with per-GPU 128
  log_interval: 75

runtime:
  memory:
    enabled: true
    step_interval: 5
  energy:
    enabled: true
    sample_interval_s: 2.0
  comms_log:
    enabled: true
  stability:
    enabled: true
